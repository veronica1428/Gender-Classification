{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Packages\n",
    "from sklearn.datasets import load_files\n",
    "from glob import glob\n",
    "#from scipy.misc import imread\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FIXED_IMG_SIZE = 227\n",
    "\n",
    "#Load the full dataset, scale the images in values between 0-1 and resize to size 227\n",
    "def load_dataset(path, target):\n",
    "    filenames = glob(path)\n",
    "    \n",
    "    if target == 0:\n",
    "        cls = [1,0] #female - 0\n",
    "    else:\n",
    "        cls = [0,1] #male - 1\n",
    "        \n",
    "    target_array = np.full(shape=(len(filenames),2), fill_value=cls, dtype='float32')\n",
    "    #target_array = np.ndarray((len(filenames),1), target)\n",
    "    \n",
    "    images = [cv2.imread(file) for file in filenames]\n",
    "    images = [cv2.resize(img, (FIXED_IMG_SIZE, FIXED_IMG_SIZE)) for img in images]\n",
    "    \n",
    "    images = np.array(images, dtype = np.uint8)\n",
    "    images = images.astype('float32')\n",
    "    images = np.multiply(images, 1.0/255.0)\n",
    "    \n",
    "    train_x, test_x, train_y, test_y = train_test_split(images, target_array, test_size=0.4)\n",
    "    valid_x, test_x, valid_y, test_y = train_test_split(test_x, test_y, test_size=0.5)\n",
    "        \n",
    "    return train_x, valid_x, test_x, train_y, valid_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Datasets\n",
      "Loaded Female Dataset\n",
      "Loaded Male Dataset\n",
      "Both datasets loaded\n",
      "Number of Training Data: 7936\n",
      "Number of Validation Data: 2646\n",
      "Number of Testing Data: 2647\n"
     ]
    }
   ],
   "source": [
    "print('Loading Datasets')\n",
    "fem_train_x, fem_valid_x, fem_test_x, fem_train_y, fem_valid_y, fem_test_y = female_test = load_dataset('data/female/*', target=1)\n",
    "print('Loaded Female Dataset')\n",
    "\n",
    "male_train_x, male_valid_x, male_test_x, male_train_y, male_valid_y, male_test_y = load_dataset('data/male/*', target=0)\n",
    "print('Loaded Male Dataset')\n",
    "\n",
    "train_len = len(fem_train_x) + len(male_train_x)\n",
    "valid_len = len(fem_valid_x) + len(male_valid_x)\n",
    "test_len = len(fem_test_x) + len(male_test_x)\n",
    "\n",
    "print('Both datasets loaded')\n",
    "print('Number of Training Data: ' + str(train_len))\n",
    "print('Number of Validation Data: ' + str(valid_len))\n",
    "print('Number of Testing Data: ' + str(test_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Inputs\n",
    "train_x = np.concatenate((fem_train_x, male_train_x), axis=0)\n",
    "valid_x = np.concatenate((fem_valid_x, male_valid_x), axis=0)\n",
    "test_x = np.concatenate((fem_test_x, male_test_x), axis=0)\n",
    "\n",
    "#targets \n",
    "train_y = np.concatenate((fem_train_y, male_train_y), axis=0)\n",
    "valid_y = np.concatenate((fem_valid_y, male_valid_y), axis=0)\n",
    "test_y = np.concatenate((fem_test_y, male_test_y), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def create_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.01), name='create_w8')\n",
    "\n",
    "def create_biases(num_filters):\n",
    "    return tf.Variable(tf.zeros(num_filters), name='create_biases') #CHECK WITH NON-ZERO VALUE                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_conv_layer(input, num_input_channels, filter_size, num_filters, conv_strides=1, conv_padding='SAME'):\n",
    "    #define weights and biases\n",
    "    weights = create_weights(shape=[filter_size, filter_size, num_input_channels, num_filters])\n",
    "    biases = create_biases(num_filters=num_filters)\n",
    "    \n",
    "    #define conv2d layer\n",
    "    conv_layer = tf.nn.conv2d(input=input, filter=weights, strides=[1,conv_strides,conv_strides,1], padding=conv_padding)\n",
    "    conv_layer += biases\n",
    "    \n",
    "    #max pooling layer\n",
    "    conv_layer = tf.nn.max_pool(value=conv_layer, ksize=[1,2,2,1], padding='SAME', strides=[1,2,2,1])\n",
    "    \n",
    "    #relu layer\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    #local response normalization layer\n",
    "    conv_layer = tf.nn.local_response_normalization(conv_layer)\n",
    "    \n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#flatten layer\n",
    "def create_flatten_layer(layer):\n",
    "    layer_shape = layer.get_shape()\n",
    "    num_elements = layer_shape[1:4].num_elements()\n",
    "    layer = tf.reshape(layer, [-1, num_elements])\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create fully connected layers\n",
    "def create_fully_conn_layer(input, num_inputs, num_outputs, use_relu=True, use_dropout=True):\n",
    "    #define weights and biases for fully connected layer\n",
    "    weights = create_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = create_biases(num_outputs)\n",
    "    \n",
    "    fconn_layer = tf.matmul(input, weights) + biases\n",
    "    if use_relu:\n",
    "        fconn_layer = tf.nn.relu(fconn_layer)\n",
    "        \n",
    "    if use_dropout:\n",
    "        fconn_layer = tf.nn.dropout(fconn_layer, keep_prob=0.2)\n",
    "        \n",
    "    return fconn_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def next_batch(batch_size, data, labels): \n",
    "    \n",
    "#     num_batches = data.shape[0] // batch_size\n",
    "#     print(data.shape[0])\n",
    "    \n",
    "#     data = data[:num_batches*batch_size]\n",
    "#     labels = labels[:num_batches*batch_size]\n",
    "    \n",
    "#     count = 1\n",
    "#     for i in range(0, data.shape[0], batch_size):\n",
    "        \n",
    "#         if count > num_batches:\n",
    "#             count += 1\n",
    "#             i = 0\n",
    "        \n",
    "#         x = data[i : i + batch_size ,:]\n",
    "#         y = np.squeeze(labels[i : i + batch_size ,:], axis=1)\n",
    "        \n",
    "#         count += 1\n",
    "#         yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_dataset_batch(batch_size, data, labels):\n",
    "    idx = np.arange(0, len(data))\n",
    "    random.shuffle(idx)\n",
    "    idx = idx[:batch_size]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    label_shuffle = [labels[i] for i in idx]\n",
    "    \n",
    "    return np.asarray(data_shuffle), np.asarray(label_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Global Variables\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate=0.05\n",
    "\n",
    "#define placeholders and input\n",
    "num_channels = 3\n",
    "num_classes = 2\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[batch_size, FIXED_IMG_SIZE, FIXED_IMG_SIZE, num_channels], name='x') \n",
    "y_true = tf.placeholder(dtype=tf.float32, shape=[batch_size, num_classes], name='y_true')\n",
    "y_true_cls = tf.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 28, 28, 96)\n",
      "(32, 14, 14, 256)\n",
      "(32, 7, 7, 256)\n",
      "(32, 12544)\n",
      "(32, 512)\n",
      "(32, 512)\n",
      "(32, 2)\n"
     ]
    }
   ],
   "source": [
    "#Define Network Architecture\n",
    "\n",
    "layer_conv1 = create_conv_layer(input=x, num_filters = 96, filter_size=7, conv_strides=4, conv_padding='VALID', num_input_channels=num_channels)\n",
    "print(layer_conv1.get_shape())\n",
    "\n",
    "layer_conv2 = create_conv_layer(input=layer_conv1, num_filters = 256, filter_size=5, conv_strides=1, \n",
    "                                conv_padding='SAME', num_input_channels=96)\n",
    "print(layer_conv2.get_shape())\n",
    "\n",
    "layer_conv3 = create_conv_layer(input=layer_conv2, num_filters = 256, filter_size=3, conv_strides=1, \n",
    "                                conv_padding='SAME', num_input_channels=256)\n",
    "print(layer_conv3.get_shape())\n",
    "\n",
    "layer_flat = create_flatten_layer(layer=layer_conv3)\n",
    "print(layer_flat.get_shape())\n",
    "\n",
    "layer_fc1 = create_fully_conn_layer(input=layer_flat, num_inputs=layer_flat.get_shape()[1:4].num_elements(),\n",
    "                                   num_outputs=512)\n",
    "print(layer_fc1.get_shape())\n",
    "\n",
    "layer_fc2 = create_fully_conn_layer(input=layer_fc1, num_inputs=512, num_outputs=512)\n",
    "print(layer_fc2.get_shape())\n",
    "\n",
    "layer_fc3 = create_fully_conn_layer(input=layer_fc2, num_inputs=512, num_outputs=num_classes, \n",
    "                                    use_relu=False, use_dropout=False)\n",
    "print(layer_fc3.get_shape())\n",
    "\n",
    "#softmax layer\n",
    "y_pred = tf.nn.softmax(layer_fc3, name='y_pred')\n",
    "y_pred_cls = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "# #define cost\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc3, labels=y_true)\n",
    "cost = tf.reduce_mean(loss)\n",
    "accuracy = tf.multiply(tf.reduce_mean(tf.cast(tf.equal(y_pred_cls, y_true_cls), tf.float32)), 100)\n",
    "\n",
    "#adam optimisation\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) #learning rate variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 :-------\n",
      "Epoch Time 32.89628338813782 (s):---- Training Loss 57.296974182128906 :------- Training Accuracy 50.0% :------ Validation Loss 61.24652862548828 :------- Validation Accuracy 71.875% :\n",
      "Epoch 2/10 :-------\n",
      "Epoch Time 32.38800287246704 (s):---- Training Loss 3.166057825088501 :------- Training Accuracy 59.375% :------ Validation Loss 3.1524415016174316 :------- Validation Accuracy 75.0% :\n",
      "Epoch 3/10 :-------\n",
      "Epoch Time 32.43212866783142 (s):---- Training Loss 0.600735068321228 :------- Training Accuracy 71.875% :------ Validation Loss 0.5584031343460083 :------- Validation Accuracy 75.0% :\n",
      "Epoch 4/10 :-------\n",
      "Epoch Time 32.40891480445862 (s):---- Training Loss 0.5846430063247681 :------- Training Accuracy 78.125% :------ Validation Loss 0.519645094871521 :------- Validation Accuracy 78.125% :\n",
      "Epoch 5/10 :-------\n",
      "Epoch Time 32.54756498336792 (s):---- Training Loss 0.5847169160842896 :------- Training Accuracy 75.0% :------ Validation Loss 0.7819252014160156 :------- Validation Accuracy 65.625% :\n",
      "Epoch 6/10 :-------\n",
      "Epoch Time 32.494181632995605 (s):---- Training Loss 0.5290142297744751 :------- Training Accuracy 78.125% :------ Validation Loss 0.5525887608528137 :------- Validation Accuracy 78.125% :\n",
      "Epoch 7/10 :-------\n",
      "Epoch Time 32.47302436828613 (s):---- Training Loss 0.4866245985031128 :------- Training Accuracy 81.25% :------ Validation Loss 0.6115458011627197 :------- Validation Accuracy 71.875% :\n",
      "Epoch 8/10 :-------\n",
      "Epoch Time 32.41285228729248 (s):---- Training Loss 0.7826476097106934 :------- Training Accuracy 62.5% :------ Validation Loss 0.6001060009002686 :------- Validation Accuracy 75.0% :\n",
      "Epoch 9/10 :-------\n",
      "Epoch Time 32.606321573257446 (s):---- Training Loss 0.5230891704559326 :------- Training Accuracy 78.125% :------ Validation Loss 0.6164214611053467 :------- Validation Accuracy 71.875% :\n",
      "Model saved in path: saved_model/gender_class_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "#Run Tensorflow session\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #sess.run(tf.initialize_all_variables()()\n",
    "\n",
    "    epoch_data = []\n",
    "    train_loss_list, valid_loss_list = [], []\n",
    "    for e in range(1, num_epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        num_batches = train_len // batch_size\n",
    "        if num_batches > 10: #limit the number of batches for faster calculations\n",
    "            num_batches = 10\n",
    "        \n",
    "        print('Epoch {}/{} :-------'.format(e, num_epochs))\n",
    "        for batch in range(0, num_batches):\n",
    "            \n",
    "            x_train_batch, y_train_batch = shuffle_dataset_batch(batch_size, data=train_x, labels=train_y)\n",
    "            x_valid_batch, y_valid_batch = shuffle_dataset_batch(batch_size, data=valid_x, labels=valid_y)\n",
    "\n",
    "            feed_dict_train = {x : x_train_batch, y_true : y_train_batch}\n",
    "            feed_dict_valid = {x : x_valid_batch, y_true : y_valid_batch}\n",
    "            sess.run(optimizer, feed_dict=feed_dict_train)\n",
    "            \n",
    "            train_loss = sess.run(cost, feed_dict=feed_dict_train)\n",
    "            #print('Batch {}/{} :-------'.format(batch, num_batches))\n",
    "  \n",
    "        valid_loss = sess.run(cost, feed_dict=feed_dict_valid)\n",
    "\n",
    "        train_accuracy = sess.run(accuracy, feed_dict=feed_dict_train)\n",
    "        valid_accuracy = sess.run(accuracy, feed_dict=feed_dict_valid)\n",
    "\n",
    "        valid_pred = sess.run(y_pred, feed_dict=feed_dict_valid)\n",
    "        valid_true_cls = sess.run(y_true_cls, feed_dict=feed_dict_valid)\n",
    "        valid_predict_cls = sess.run(y_pred_cls, feed_dict=feed_dict_valid)\n",
    "        \n",
    "        end_time = time.time() \n",
    "        \n",
    "        epoch_data.append((valid_pred, valid_predict_cls, valid_true_cls))\n",
    "        \n",
    "        train_loss_list.append(train_loss)\n",
    "        valid_loss_list.append(valid_loss)\n",
    "        print('Epoch Time {} (s):----'.format(end_time - start_time),\n",
    "              'Training Loss {} :-------'.format(train_loss),\n",
    "              'Training Accuracy {}% :------'.format(train_accuracy),\n",
    "              'Validation Loss {} :-------'.format(valid_loss),\n",
    "              'Validation Accuracy {}% :'.format(valid_accuracy))\n",
    "              #'Validation True Class / Pred Class {}/{} :--'.format(valid_true_cls, valid_predict_cls)))\n",
    "    \n",
    "    saved_path = saver.save(sess, 'saved_model/gender_class_model.ckpt')\n",
    "    print('Model saved in path: {}'.format(saved_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saved_model/gender_class_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saved_model/gender_class_model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored\n"
     ]
    }
   ],
   "source": [
    "#restore the model\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver.restore(sess, 'saved_model/gender_class_model.ckpt')\n",
    "    print('Model restored')\n",
    "    \n",
    "    output_df = pd.DataFrame()\n",
    "    epoch = epoch_data[len(epoch_data)-1] #Getting the last epoch, last batch data\n",
    "\n",
    "    output_df['Female Prob(0)'] = np.squeeze(epoch[0][:,0])\n",
    "    output_df['Male Prob(1)'] = np.squeeze(epoch[0][:,1])\n",
    "    output_df['Predicted Class'] = np.squeeze(np.expand_dims(epoch[1], axis=1))\n",
    "    output_df['True Class'] = np.squeeze(np.expand_dims(epoch[2], axis=1))\n",
    "    \n",
    "    output_df.to_csv('output.csv')\n",
    "    \n",
    "# #tf.get_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fa3bbbda0b8>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa3fd2708d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot train and validation loss curve to understand the performance of the model\n",
    "plt.plot(range(1,10), train_loss_list, label='Train Loss')\n",
    "plt.plot(range(1,10), valid_loss_list, label='Valid Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cost per epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMsAAACGCAYAAABzPX6BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWZLDYpb0H1t11gTli1R4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb2d9be0e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_alt.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#detect human face boundaries\n",
    "# def face_detector(img_path):\n",
    "#     img = cv2.imread(img_path)\n",
    "#     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     faces = face_cascade.detectMultiScale(gray)\n",
    "    \n",
    "#     return len(faces) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
